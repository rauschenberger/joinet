[{"path":"/articles/article.html","id":"predicting-correlated-outcomes-from-molecular-data","dir":"Articles","previous_headings":"","what":"Predicting correlated outcomes from molecular data","title":"Multivariate Elastic Net Regression","text":"Armin Rauschenberger1,*~^{1,*} Enrico Glaab1~^{1} 1^1Luxembourg Centre Systems Biomedicine (LCSB), University Luxembourg, Esch-sur-Alzette, Luxembourg. *^{*}correspondence addressed.","code":""},{"path":"/articles/article.html","id":"abstract","dir":"Articles","previous_headings":"Predicting correlated outcomes from molecular data","what":"Abstract","title":"Multivariate Elastic Net Regression","text":"Multivariate (multi-target) regression potential outperform univariate (single-target) regression predicting correlated outcomes, frequently occur biomedical clinical research. implement multivariate lasso ridge regression using stacked generalization. flexible approach leads predictive interpretable models high-dimensional settings, single estimate input–output effect. simulation, compare predictive performance several state---art methods multivariate regression. application, use clinical genomic data predict multiple motor non-motor symptoms Parkinson’s disease patients. conclude stacked multivariate regression, adaptations, competitive method predicting correlated outcomes. R package joinet available GitHub CRAN.","code":""},{"path":"/articles/article.html","id":"full-text-open-access","dir":"Articles","previous_headings":"Predicting correlated outcomes from molecular data","what":"Full text (open access)","title":"Multivariate Elastic Net Regression","text":"Armin Rauschenberger  Enrico Glaab  (2021). “Predicting correlated outcomes molecular data”. Bioinformatics 37(21):3889–3895. doi: 10.1093/bioinformatics/btab576. (Click access PDF.)","code":""},{"path":"/articles/script.html","id":"simulation","dir":"Articles","previous_headings":"","what":"Simulation","title":"Multivariate Elastic Net Regression","text":"","code":"#<<start>> library(joinet)  grid <- list() grid$rho_x <- c(0.00,0.10,0.30) grid$rho_b <- c(0.00,0.50,0.90) delta <- 0.8 grid <- expand.grid(grid) grid <- rbind(grid,grid,grid) grid$p <- rep(c(10,500,500),each=nrow(grid)/3) grid$nzero <- rep(c(5,5,100),each=nrow(grid)/3) grid <- grid[rep(1:nrow(grid),times=10),]  n0 <- 100; n1 <- 10000 n <- n0 + n1 q <- 3 foldid.ext <- rep(c(0,1),times=c(n0,n1))  loss <- list(); cor <- numeric(); seed <- list() set.seed(1) # new for(i in seq_len(nrow(grid))){   p <- grid$p[i]   #set.seed(i) # old   seed[[i]] <- .Random.seed # new   cat(\"i =\",i,\"\\n\")      #--- features ---   mean <- rep(0,times=p)   sigma <- matrix(grid$rho_x[i],nrow=p,ncol=p)   diag(sigma) <- 1   X <- mvtnorm::rmvnorm(n=n,mean=mean,sigma=sigma)      #--- effects --- (multivariate Gaussian)   mean <- rep(0,times=q)   sigma <- matrix(data=grid$rho_b[i],nrow=q,ncol=q)   diag(sigma) <- 1   beta <- mvtnorm::rmvnorm(n=p,mean=mean,sigma=sigma)   #beta <- 1*apply(beta,2,function(x) x>(sort(x,decreasing=TRUE)[grid$nzero[i]])) # old (either zero or one)   beta <- 1*apply(beta,2,function(x) ifelse(x>sort(x,decreasing=TRUE)[grid$nzero[i]],x,0)) # new (either zero or non-zero)      #-- effects --- (multivariate binomial)   #sigma <- matrix(grid$rho_b[i],nrow=q,ncol=q); diag(sigma) <- 1   #beta <- bindata::rmvbin(n=p,margprob=rep(grid$prop[i],times=q),bincorr=sigma)      #--- outcomes ---   signal <- scale(X%*%beta)   signal[is.na(signal)] <- 0   noise <- matrix(rnorm(n*q),nrow=n,ncol=q)   Y <- sqrt(delta)*signal + sqrt(1-delta)*noise   # binomial: Y <- round(exp(Y)/(1+exp(Y)))   cors <- stats::cor(Y)   diag(cors) <- NA   cor[i] <- mean(cors,na.rm=TRUE)      #--- holdout ---   alpha.base <- 1*(grid$nzero[i] <= 10) # sparse vs dense   compare <- TRUE   loss[[i]] <- tryCatch(expr=cv.joinet(Y=Y,X=X,family=\"gaussian\",compare=compare,foldid.ext=foldid.ext,alpha.base=alpha.base,alpha.meta=1,times=TRUE,sign=1),error=function(x) NA) } save(grid,loss,cor,seed,file=\"results/simulation.RData\") writeLines(text=capture.output(utils::sessionInfo(),cat(\"\\n\"),sessioninfo::session_info()),con=\"results/info_sim.txt\") #<<start>> load(\"results/simulation.RData\")  cond <- lapply(loss,length)==2 if(any(!cond)){   warning(\"At least one error.\",call.=FALSE)   grid <- grid[cond,]; loss <- loss[cond] }  #--- computation time --- time <- sapply(loss,function(x) unlist(x$time)) #round(sort(colMeans(apply(time,1,function(x) x/time[\"meta\",]))),digits=1) sort(round(rowMeans(time),digits=1))  #--- average --- loss <- lapply(loss,function(x) x$loss)  #prop <- sapply(loss[cond],function(x) rowMeans(100*x/matrix(x[\"none\",],nrow=nrow(x),ncol=ncol(x),byrow=TRUE))[-nrow(x)]) # old (first re-scale, then average) mean <- sapply(loss,function(x) rowMeans(x)) # new (first average) prop <- 100*mean[rownames(mean)!=\"none\",]/matrix(mean[\"none\",],nrow=nrow(mean)-1,ncol=ncol(mean),byrow=TRUE) # new (then re-scale)  mode <- ifelse(grid$p==10,\"ld\",ifelse(grid$nzero==5,\"hd-s\",\"hd-d\")) set <- as.numeric(sapply(rownames(grid),function(x) strsplit(x,split=\"\\\\.\")[[1]][[1]]))  #--- mean rank --- mult <- rownames(prop)!=\"base\" sort(round(rowMeans(apply(prop[mult,mode==\"ld\"],2,rank)),digits=1)) sort(round(rowMeans(apply(prop[mult,mode==\"hd-s\"],2,rank)),digits=1)) sort(round(rowMeans(apply(prop[mult,mode==\"hd-d\"],2,rank)),digits=1))  #--- testing --- apply(prop[mult,],1,function(x) sum(tapply(X=x-prop[\"base\",],INDEX=set,FUN=function(x) wilcox.test(x,alternative=\"less\",exact=FALSE)$p.value<0.05),na.rm=TRUE))  colSums(tapply(X=prop[\"meta\",]-prop[\"base\",],INDEX=list(set=set,mode=mode),FUN=function(x) wilcox.test(x,alternative=\"less\",exact=FALSE)$p.value<0.05),na.rm=TRUE) colSums(tapply(X=prop[\"spls\",]-prop[\"base\",],INDEX=list(set=set,mode=mode),FUN=function(x) wilcox.test(x,alternative=\"less\",exact=FALSE)$p.value<0.05),na.rm=TRUE) beta <- sapply(unique(set),function(i) rowMeans(prop[,set==i])) cor <- sapply(unique(set),function(i) mean(cor[set==i]))  rownames(beta)[rownames(beta)==\"mnorm\"] <- \"mvn\" sign <- apply(beta,2,function(x) sign(x[\"base\"]-x)) #min <- apply(beta,2,function(x) which.min(x)) # incorrect (old) min <- apply(beta,2,function(x) which(x==min(x))) # correct (new) beta <- format(round(beta,digits=1),trim=TRUE) beta[sign<=0] <- paste0(\"\\\\textcolor{gray}{\",beta[sign<=0],\"}\") index <- cbind(min,1:ncol(beta)) beta[index] <- paste0(\"\\\\underline{\",beta[index],\"}\") unique <- unique(grid) info <- format(round(cbind(\"$\\\\rho_x$\"=unique$rho_x,\"$\\\\rho_b$\"=unique$rho_b,\"$\\\\rho_y$\"=cor),digits=1)) temp <- paste0(\"\\\\cran{\",sapply(rownames(beta),function(x) switch(x,base=\"glmnet\",meta=\"joinet\",mvn=\"glmnet\",mars=\"earth\",spls=\"spls\",mrce=\"MRCE\",map=\"remMap\",mrf=\"MultivariateRandomForest\",sier=\"SiER\",mcen=\"mcen\",gpm=\"GPM\",rmtl=\"RMTL\",mtps=\"MTPS\",NULL)),\"}\") temp[1] <- paste0(temp[1],\"$^1$\") temp[3] <- paste0(temp[3],\"$^2$\") temp[8] <- \"\\\\href{https://CRAN.R-project.org/package=MultivariateRandomForest}{\\\\texttt{MRF}}$^3$\" rownames(beta) <- paste0(\"\\\\begin{sideways}\",temp,\"\\\\end{sideways}\") xtable <- xtable::xtable(cbind(info,t(beta)),align=paste0(\"rccc\",paste0(rep(\"c\",times=nrow(beta)),collapse=\"\"),collapse=\"\"),caption=\"\") xtable::print.xtable(xtable,comment=FALSE,floating=TRUE,sanitize.text.function=identity,hline.after=c(0,9,18,ncol(beta)),include.rownames=FALSE,size=\"\\\\footnotesize\",caption.placement=\"top\")"},{"path":"/articles/script.html","id":"application","dir":"Articles","previous_headings":"","what":"Application","title":"Multivariate Elastic Net Regression","text":"","code":"# clinical features X <- read.csv(\"data/PPMI_Baseline_Data_02Jul2018.csv\",row.names=\"PATNO\",na.strings=c(\".\",\"\")) colnames(X) <- tolower(colnames(X)) X <- X[X$apprdx==1,] # Parkinson's disease X[c(\"site\",\"apprdx\",\"event_id\",\"symptom5_comment\")] <- NULL for(i in seq_len(ncol(X))){   if(is.factor(X[[i]])){levels(X[[i]]) <- paste0(\"-\",levels(X[[i]]))} } 100*mean(is.na(X)) # proportion missingness x <- lapply(seq_len(1),function(x) missRanger::missRanger(data=X,pmm.k=3,         num.trees=100,verbose=0,seed=1)) x <- lapply(x,function(x) model.matrix(~.-1,data=x))  # genomic features load(\"data/ppmi_rnaseq_bl_pd_hc-2019-01-11.Rdata\",verbose=TRUE) counts <- t(ppmi_rnaseq_bl_pdhc) mean(grepl(pattern=\"ENSG0000|ENSGR0000\",x=colnames(counts))) cond <- apply(counts,2,function(x) sd(x)>0) & !grepl(pattern=\"ENSG0000|ENSGR0000\",x=colnames(counts)) Z <- palasso:::.prepare(counts[,cond],filter=10,cutoff=\"knee\")$X  # outcome Y <- read.csv(\"data/PPMI_Year_1-3_Data_02Jul2018.csv\",na.strings=\".\") Y <- Y[Y$APPRDX==1 & Y$EVENT_ID %in% c(\"V04\",\"V06\",\"V08\"),] colnames(Y)[colnames(Y)==\"updrs_totscore\"] <- \"updrs\" vars <- c(\"moca\",\"quip\",\"updrs\",\"gds\",\"scopa\",\"ess\",\"bjlot\",\"rem\") # too few levels: \"NP1HALL\",\"NP1DPRS\" Y <- Y[,c(\"EVENT_ID\",\"PATNO\",vars)] Y <- reshape(Y,idvar=\"PATNO\",timevar=\"EVENT_ID\",direction=\"wide\") rownames(Y) <- Y$PATNO; Y$PATNO <- NULL  # overlap names <- Reduce(intersect,list(rownames(X),rownames(Y),rownames(Z))) Z <- Z[names,] Y <- Y[names,] Y <- sapply(vars,function(x) Y[,grepl(pattern=x,x=colnames(Y))],simplify=FALSE) for(i in seq_along(Y)){   colnames(Y[[i]]) <- c(\"V04\",\"V06\",\"V08\") } x <- lapply(x,function(x) x[names,]); rm(names) X <- x[[1]]; rm(x) # impute multiple times!  # inversion for positive correlation Y$moca <- -Y$moca # \"wrong\" sign Y$bjlot <- -Y$bjlot # \"wrong\" sign sapply(Y,function(x) range(unlist(x),na.rm=TRUE))  save(Y,X,Z,file=\"results/data.RData\") writeLines(text=capture.output(utils::sessionInfo(),cat(\"\\n\"),sessioninfo::session_info()),con=\"results/info_dat.txt\") load(\"results/data.RData\",verbose=TRUE)  #grDevices::pdf(file=\"manuscript/figure_COR.pdf\",width=6,height=3) grDevices::postscript(file=\"manuscript/figure_COR.eps\",width=6,height=3) graphics::par(mar=c(0.5,3,2,0.5)) graphics::layout(mat=matrix(c(1,2),nrow=1,ncol=2),width=c(0.2,0.8))  # correlation between years cor <- cbind(sapply(Y,function(x) cor(x[,1],x[,2],use=\"complete.obs\",method=\"spearman\")), sapply(Y,function(x) cor(x[,2],x[,3],use=\"complete.obs\",method=\"spearman\")), sapply(Y,function(x) cor(x[,1],x[,3],use=\"complete.obs\",method=\"spearman\"))) colnames(cor) <- c(\"1-2\",\"2-3\",\"1-3\") cor <- rowMeans(cor) joinet:::plot.matrix(cor,range=c(-3,3),margin=1,cex=0.7)  # correlation between variables cor <- 1/3*cor(sapply(Y,function(x) x[,1]),use=\"complete.obs\",method=\"spearman\")+ 1/3*cor(sapply(Y,function(x) x[,2]),use=\"complete.obs\",method=\"spearman\")+ 1/3*cor(sapply(Y,function(x) x[,3]),use=\"complete.obs\",method=\"spearman\") joinet:::plot.matrix(cor,range=c(-3,3),margin=c(1,2),cex=0.7) grDevices::dev.off()  # other information sapply(Y,colMeans,na.rm=TRUE) # increasing values sapply(Y,function(x) apply(x,2,sd,na.rm=TRUE)) # increasing variance sapply(Y,function(x) colSums(is.na(x))) # increasing numbers of NAs #<<start>> library(joinet)  set.seed(1) load(\"results/data.RData\",verbose=TRUE)  set.seed(1) foldid.ext <- rep(1:5,length.out=nrow(Y$moca)) foldid.int <- rep(rep(1:10,each=5),length.out=nrow(Y$moca)) table(foldid.ext,foldid.int)  #- - - - - - - - - - - - - #- - internal coaching - - #- - - - - - - - - - - - -  table <- list() table$alpha <- c(\"lasso\",\"ridge\") table$data <- c(\"clinic\",\"omics\",\"both\") table$var <- names(Y) table <- rev(expand.grid(table,stringsAsFactors=FALSE))  loss <- fit <- list() for(i in seq_len(nrow(table))){   cat(rep(\"*\",times=5),\"setting\",i,rep(\"*\",times=5),\"\\n\")   y <- Y[[table$var[i]]]   x <- list(clinic=X,omics=Z,both=cbind(X,Z))[[table$data[i]]]   alpha <- 1*(table$alpha[i]==\"lasso\")   loss[[i]] <- cv.joinet(Y=y,X=x,alpha.base=alpha,foldid.ext=foldid.ext,           foldid.int=foldid.int,sign=1) # add joinet::   #fit[[i]] <- joinet(Y=y,X=x,alpha.base=alpha,foldid=foldid.int,sign=1) }  save(table,loss,file=\"results/internal.RData\")  #- - - - - - - - - - - - - #- - external coaching - - #- - - - - - - - - - - - -  table <- list() temp <- utils::combn(x=names(Y),m=2) table$comb <- paste0(temp[1,],\"-\",temp[2,]) table$step <-  c(\"V04\",\"V06\",\"V08\") table$alpha <- c(\"lasso\",\"ridge\") table$data <- c(\"clinic\",\"omics\",\"both\") table <- rev(expand.grid(table,stringsAsFactors=FALSE)) temp <- strsplit(table$comb,split=\"-\"); table$comb <- NULL table$var1 <- sapply(temp,function(x) x[[1]]) table$var2 <- sapply(temp,function(x) x[[2]])  loss <- fit <- list() for(i in seq_len(nrow(table))){   cat(rep(\"*\",times=5),\"setting\",i,rep(\"*\",times=5),\"\\n\")   y <- cbind(Y[[table$var1[i]]][,table$step[i]],              Y[[table$var2[i]]][,table$step[i]])   x <- list(clinic=X,omics=Z,both=cbind(X,Z))[[table$data[i]]]   alpha <- 1*(table$alpha[i]==\"lasso\")   loss[[i]] <- cv.joinet(Y=y,X=x,alpha.base=alpha,                 foldid.ext=foldid.ext,foldid.int=foldid.int,sign=1) # add joinet::   #fit[[i]] <- joinet(Y=y,X=x,alpha.base=alpha,foldid=foldid.int,sign=1) }  save(table,loss,file=\"results/external.RData\") writeLines(text=capture.output(utils::sessionInfo(),cat(\"\\n\"),sessioninfo::session_info()),con=\"results/info_app.txt\") load(\"results/internal.RData\")  # standardised loss vars <- unique(table$var) former <- t(sapply(loss,function(x) x[\"base\",])) min <- sapply(vars,function(x) min(former[table$var==x,])) max <- sapply(vars,function(x) max(former[table$var==x,])) index <- match(x=table$var,table=vars) former <- (former-min[index])/(max[index]-min[index]) dimnames(former) <- list(table$var,seq_len(3))  # percentage change change <- t(sapply(loss,function(x) 100*(x[\"meta\",]-x[\"base\",])/x[\"base\",])) dimnames(change) <- list(table$var,c(\"1st\",\"2nd\",\"3rd\"))  # overview #grDevices::pdf(file=\"manuscript/figure_INT.pdf\",width=6,height=3,pointsize=12) grDevices::postscript(file=\"manuscript/figure_INT.eps\",width=6,height=3,pointsize=12) graphics::par(mfrow=c(2,3),mar=c(0.1,3,2,0.1),oma=c(0,1.1,1,0)) for(alpha in c(\"lasso\",\"ridge\")){   for(data in c(\"clinic\",\"omics\",\"both\")){     cond <- table$alpha==alpha & table$data==data     joinet:::plot.matrix(X=change[cond,],range=c(-50,50),cex=0.7)     #graphics::title(main=paste0(alpha,\"-\",data),col.main=\"red\",line=0) # check     if(alpha==\"lasso\"){graphics::mtext(text=data,side=3,line=1.5,cex=0.8)}     if(data==\"clinic\"){graphics::mtext(text=alpha,side=2,line=3,cex=0.8)}   } } grDevices::dev.off()  TEMP <- tapply(X=rowMeans(change),INDEX=table$var,FUN=mean)[vars] mean(change<0) round(tapply(X=rowMeans(change),INDEX=table$data,FUN=mean),digits=2) round(tapply(X=rowMeans(change),INDEX=table$alpha,FUN=mean),digits=2) round(colMeans(change),digits=2) load(\"results/external.RData\")  data <- c(\"clinic\",\"omics\",\"both\") alpha <- c(\"lasso\",\"ridge\") step <- c(\"V04\",\"V06\",\"V08\")  # percentage change change <- t(sapply(loss,function(x) 100*(x[\"meta\",]-x[\"base\",])/x[\"base\",]))  # overview vars <- unique(c(table$var1,table$var2)) temp <- matrix(NA,nrow=length(vars),ncol=length(vars),dimnames=list(vars,vars)) array <- array(data=list(temp),dim=c(3,2,3),dimnames=list(data,alpha,step)) #grDevices::pdf(file=\"manuscript/figure_EXT.pdf\",width=7.5,height=10,pointsize=14) grDevices::postscript(file=\"manuscript/figure_EXT.eps\",width=7.5,height=10,pointsize=14) graphics::par(mfrow=c(6,3),mar=c(0.1,2.5,2.5,0.1),oma=c(0,1,2,0)) for(i in data){   for(j in alpha){     for(k in step){       cond <- table$data==i & table$alpha==j & table$step==k       array[i,j,k][[1]][cbind(table$var1,table$var2)[cond,]] <- change[cond,1]       array[i,j,k][[1]][cbind(table$var2,table$var1)[cond,]] <- change[cond,2]       joinet:::plot.matrix(array[i,j,k][[1]],margin=0,las=2,range=c(-20,20),cex=0.6)       #graphics::title(main=paste0(i,\"-\",j,\"-\",k),col.main=\"red\",line=0) # check       if(i==\"clinic\" & j==\"lasso\"){graphics::mtext(text=ifelse(k==\"V04\",\"1st\",ifelse(k==\"V06\",\"2nd\",\"3rd\")),side=3,line=2.5,cex=0.8)}       if(k==\"V04\"){graphics::mtext(text=paste0(i,\"-\",j),side=2,line=2.5,cex=0.8)}     }   } } grDevices::dev.off()  # check i <- sample(seq_len(nrow(table)),size=1) table[i,] x <- loss[[i]] 100*(x[\"meta\",]-x[\"base\",])/x[\"base\",] #grDevices::pdf(file=\"manuscript/figure_ALL.pdf\",height=3,width=6) grDevices::postscript(file=\"manuscript/figure_ALL.eps\",height=3,width=6)  graphics::par(mar=c(0.5,3,2,0.5)) graphics::layout(mat=matrix(c(1,2),nrow=1,ncol=2),width=c(0.2,0.8)) joinet:::plot.matrix(as.matrix(TEMP),margin=1,las=1,range=c(-20,20),cex=0.7)  sum(unlist(array)<0,na.rm=TRUE)/sum(!is.na(unlist(array))) means <- apply(array,c(1,2,3),function(x) mean(x[[1]],na.rm=TRUE)) lapply(seq_len(3),function(x) apply(means,x,mean)) mean <- 1/length(array)*Reduce(f=\"+\",x=array) joinet:::plot.matrix(mean,margin=1,las=1,range=c(-20,20),cex=0.7) # rows: target variable, columns: coaching variable  grDevices::dev.off() #grDevices::pdf(file=\"manuscript/figure_DIF.pdf\",height=1.2,width=5) grDevices::postscript(file=\"manuscript/figure_DIF.eps\",height=1.2,width=5)  load(\"results/internal.RData\") vars <- unique(table$var) base <- t(sapply(loss,function(x) 100*(x[\"base\",]-x[\"none\",])/x[\"none\",])) meta <- t(sapply(loss,function(x) 100*(x[\"meta\",]-x[\"none\",])/x[\"none\",])) dimnames(meta) <- dimnames(base) <- list(table$var,c(\"1st\",\"2nd\",\"3rd\")) standard <- tapply(X=rowMeans(base),INDEX=table$var,FUN=mean)[vars] internal <- tapply(X=rowMeans(meta),INDEX=table$var,FUN=mean)[vars]  load(\"results/external.RData\") vars <- unique(c(table$var1,table$var2)) base <- meta <- list() for(i in seq_len(2)){   base[[i]] <- sapply(loss,function(x) 100*(x[\"base\",i]-x[\"none\",i])/x[\"none\",i])   meta[[i]] <- sapply(loss,function(x) 100*(x[\"meta\",i]-x[\"none\",i])/x[\"none\",i]) } index <- c(table$var1,table$var2) base <- unlist(base); meta <- unlist(meta) #standard <- tapply(X=base,INDEX=index,FUN=mean)[vars] external <- tapply(X=meta,INDEX=index,FUN=mean)[vars]  matrix <- round(rbind(standard,internal,external),digits=2) rownames(matrix) <- c(\"\",\"\",\"\") graphics::par(mfrow=c(1,1),mar=c(0.5,3,1.5,1)) joinet:::plot.matrix(matrix,margin=c(1,2),las=1,range=c(-100,0),cex=0.7,digits=3)  grDevices::dev.off()"},{"path":"/articles/vignette.html","id":"installation","dir":"Articles","previous_headings":"","what":"Installation","title":"Multivariate Elastic Net Regression","text":"Install current release CRAN: install latest development version GitHub:","code":"install.packages(\"joinet\") #install.packages(\"devtools\") devtools::install_github(\"rauschenberger/joinet\")"},{"path":"/articles/vignette.html","id":"initialisation","dir":"Articles","previous_headings":"","what":"Initialisation","title":"Multivariate Elastic Net Regression","text":"Load attach package: access documentation:","code":"library(joinet) ?joinet help(joinet) browseVignettes(\"joinet\")"},{"path":"/articles/vignette.html","id":"simulation","dir":"Articles","previous_headings":"","what":"Simulation","title":"Multivariate Elastic Net Regression","text":"n samples, simulate p inputs (features, covariates) q outputs (outcomes, responses). assume high-dimensional inputs (p ≫\\ggn) low-dimensional outputs (q ≪\\lln). simulate p inputs multivariate normal distribution. mean, use p-dimensional vector mu, elements equal zero. covariance, use p ×\\timesp matrix Sigma, entry row ii column jj equals rho|−j|^{|-j|}. parameter rho determines strength correlation among inputs, small rho leading weak correlations, large rho leading strong correlations (0 < rho < 1). input matrix X n rows p columns. simulate input-output effects independent Bernoulli distributions. parameter pi determines number effects, small pi leading effects, large pi leading many effects (0 < pi < 1). scalar alpha represents intercept, p-dimensional vector beta represents slopes. intercept alpha, slopes beta inputs X, calculate linear predictor, n-dimensional vector eta. Rescale linear predictor make effects weaker stronger. Set argument family \"gaussian\", \"binomial\", \"poisson\" define distribution. n times p matrix Y represents outputs. assume outcomes positively correlated.","code":"n <- 100 q <- 2 p <- 500 mu <- rep(0,times=p) rho <- 0.90 Sigma <- rho^abs(col(diag(p))-row(diag(p))) X <- MASS::mvrnorm(n=n,mu=mu,Sigma=Sigma) pi <- 0.01 alpha <- 0 beta <- rbinom(n=p,size=1,prob=pi) eta <- alpha + X %*% beta eta <- 1.5*scale(eta) family <- \"gaussian\"  if(family==\"gaussian\"){   mean <- eta   Y <- replicate(n=q,expr=rnorm(n=n,mean=mean)) }  if(family==\"binomial\"){   prob <- 1/(1+exp(-eta))   Y <- replicate(n=q,expr=rbinom(n=n,size=1,prob=prob)) }  if(family==\"poisson\"){   lambda <- exp(eta)   Y <- replicate(n=q,expr=rpois(n=n,lambda=lambda)) }  cor(Y)"},{"path":"/articles/vignette.html","id":"application","dir":"Articles","previous_headings":"","what":"Application","title":"Multivariate Elastic Net Regression","text":"function joinet fits univariate multivariate regression. Set argument alpha.base 0 (ridge) 1 (lasso). Standard methods available. function predict returns predicted values univariate (base) multivariate (meta) models. function coef returns estimated intercepts (alpha) slopes (beta) multivariate model (input-output effects). function weights returns weights stacking (output-output effects). function cv.joinet compares predictive performance univariate (base) multivariate (meta) regression nested cross-validation. argument type.measure determines loss function.","code":"object <- joinet(Y=Y,X=X,family=family) predict(object,newx=X)  coef(object)  weights(object) cv.joinet(Y=Y,X=X,family=family) ##          [,1]     [,2] ## base 1.196388 1.606624 ## meta 1.170356 1.262356 ## none 3.249320 3.496644"},{"path":"/articles/vignette.html","id":"reference","dir":"Articles","previous_headings":"","what":"Reference","title":"Multivariate Elastic Net Regression","text":"Armin Rauschenberger  Enrico Glaab  (2021). “Predicting correlated outcomes molecular data”. Bioinformatics 37(21):3889–3895. doi: 10.1093/bioinformatics/btab576.","code":""},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Armin Rauschenberger. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Armin Rauschenberger Enrico Glaab (2021). \"Predicting correlated outcomes molecular data\". Bioinformatics 37(21):3889–3895. https://doi.org/10.1093/bioinformatics/btab576","code":"@Article{,   title = {Predicting correlated outcomes from molecular data},   author = {Armin Rauschenberger and Enrico Glaab},   journal = {Bioinformatics},   year = {2021},   volume = {37},   number = {21},   pages = {3889–3895},   doi = {10.1093/bioinformatics/btab576}, }"},{"path":"/index.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Multivariate Elastic Net Regression","text":"Multivariate elastic net regression stacked generalisation (extending R package glmnet).","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Multivariate Elastic Net Regression","text":"Install current release CRAN: latest development version GitHub:","code":"install.packages(\"joinet\") #install.packages(\"remotes\") remotes::install_github(\"rauschenberger/joinet\")"},{"path":"/index.html","id":"reference","dir":"","previous_headings":"","what":"Reference","title":"Multivariate Elastic Net Regression","text":"Armin Rauschenberger  Enrico Glaab  (2021). “Predicting correlated outcomes molecular data”. Bioinformatics 37(21):3889–3895. doi: 10.1093/bioinformatics/btab576.","code":""},{"path":"/index.html","id":"disclaimer","dir":"","previous_headings":"","what":"Disclaimer","title":"Multivariate Elastic Net Regression","text":"R package joinet implements multivariate elastic net regression stacked generalisation (Rauschenberger & Glaab, 2021). Copyright © 2019 Armin Rauschenberger, University Luxembourg, Luxembourg Centre Systems Biomedicine (LCSB), Biomedical Data Science (BDS) program free software: can redistribute /modify terms GNU General Public License published Free Software Foundation, either version 3 License, (option) later version. program distributed hope useful, WITHOUT WARRANTY; without even implied warranty MERCHANTABILITY FITNESS PARTICULAR PURPOSE. See GNU General Public License details. received copy GNU General Public License along program. , see http://www.gnu.org/licenses/.","code":""},{"path":"/reference/coef.joinet.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Coefficients — coef.joinet","title":"Extract Coefficients — coef.joinet","text":"Extracts pooled coefficients. (meta learners linearly combines coefficients base learners.)","code":""},{"path":"/reference/coef.joinet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Coefficients — coef.joinet","text":"","code":"# S3 method for class 'joinet' coef(object, ...)"},{"path":"/reference/coef.joinet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Coefficients — coef.joinet","text":"object joinet object ... arguments (applicable)","code":""},{"path":"/reference/coef.joinet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Coefficients — coef.joinet","text":"function returns pooled coefficients. slot alpha contains intercepts vector length \\(q\\), slot beta contains slopes matrix \\(p\\) rows (inputs) \\(q\\) columns.","code":""},{"path":"/reference/coef.joinet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Coefficients — coef.joinet","text":"","code":"if (FALSE) { # \\dontrun{ n <- 50; p <- 100; q <- 3 X <- matrix(rnorm(n*p),nrow=n,ncol=p) Y <- replicate(n=q,expr=rnorm(n=n,mean=rowSums(X[,1:5]))) object <- joinet(Y=Y,X=X) coef <- coef(object)} # }"},{"path":"/reference/cv.joinet.html","id":null,"dir":"Reference","previous_headings":"","what":"Model comparison — cv.joinet","title":"Model comparison — cv.joinet","text":"Compares univariate multivariate regression.","code":""},{"path":"/reference/cv.joinet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model comparison — cv.joinet","text":"","code":"cv.joinet(   Y,   X,   family = \"gaussian\",   nfolds.ext = 5,   nfolds.int = 10,   foldid.ext = NULL,   foldid.int = NULL,   type.measure = \"deviance\",   alpha.base = 1,   alpha.meta = 1,   compare = FALSE,   mice = FALSE,   cvpred = FALSE,   times = FALSE,   ... )"},{"path":"/reference/cv.joinet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model comparison — cv.joinet","text":"Y outputs: numeric matrix \\(n\\) rows (samples) \\(q\\) columns (outputs) X inputs: numeric matrix \\(n\\) rows (samples) \\(p\\) columns (inputs) family distribution: vector length \\(1\\) \\(q\\) entries \"gaussian\", \"binomial\" \"poisson\" nfolds.ext number external folds nfolds.int number internal folds foldid.ext external fold identifiers: vector length \\(n\\) entries \\(1\\) nfolds.ext; NULL foldid.int internal fold identifiers: vector length \\(n\\) entries \\(1\\) nfolds.int; NULL type.measure loss function: vector length \\(1\\) \\(q\\) entries \"deviance\", \"class\", \"mse\" \"mae\" (see cv.glmnet) alpha.base elastic net mixing parameter base learners: numeric \\(0\\) (ridge) \\(1\\) (lasso) alpha.meta elastic net mixing parameter meta learners: numeric \\(0\\) (ridge) \\(1\\) (lasso) compare experimental arguments: character vector entries \"mnorm\", \"spls\", \"mrce\", \"sier\", \"mtps\", \"rmtl\", \"gpm\" others (requires packages spls, MRCE, SiER, MTPS, RMTL GPM) mice missing data imputation: logical (mice=TRUE requires package mice) cvpred return cross-validated predictions: logical times measure computation time: logical ... arguments passed glmnet cv.glmnet","code":""},{"path":"/reference/cv.joinet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model comparison — cv.joinet","text":"function returns matrix \\(q\\) columns, including cross-validated loss univariate models (base), multivariate models (meta), intercept-models (none).","code":""},{"path":[]},{"path":"/reference/joinet-package.html","id":null,"dir":"Reference","previous_headings":"","what":"Multivariate Elastic Net Regression — joinet-package","title":"Multivariate Elastic Net Regression — joinet-package","text":"R package joinet implements multivariate ridge lasso regression using stacked generalisation. multivariate regression typically outperforms univariate regression predicting correlated outcomes. provides predictive interpretable models high-dimensional settings.","code":""},{"path":"/reference/joinet-package.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Multivariate Elastic Net Regression — joinet-package","text":"Use function joinet model fitting. Type library(joinet) ?joinet help(\"joinet)\" open help file. See vignette examples. Type vignette(\"joinet\") browseVignettes(\"joinet\") open vignette.","code":""},{"path":"/reference/joinet-package.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Multivariate Elastic Net Regression — joinet-package","text":"Armin Rauschenberger Enrico Glaab (2021) \"Predicting correlated outcomes molecular data\" Bioinformatics 37(21):3889–3895. doi:10.1093/bioinformatics/btab576  (Click access PDF.)","code":""},{"path":[]},{"path":"/reference/joinet-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Multivariate Elastic Net Regression — joinet-package","text":"Maintainer: Armin Rauschenberger armin.rauschenberger@uni.lu (ORCID)","code":""},{"path":"/reference/joinet-package.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Multivariate Elastic Net Regression — joinet-package","text":"","code":"#>  #>  #>  #>  #>  if (FALSE) { # \\dontrun{ #--- data simulation --- n <- 50; p <- 100; q <- 3 X <- matrix(rnorm(n*p),nrow=n,ncol=p) Y <- replicate(n=q,expr=rnorm(n=n,mean=rowSums(X[,1:5]))) # n samples, p inputs, q outputs  #--- model fitting --- object <- joinet(Y=Y,X=X) # slot \"base\": univariate # slot \"meta\": multivariate  #--- make predictions --- y_hat <- predict(object,newx=X) # n x q matrix \"base\": univariate # n x q matrix \"meta\": multivariate   #--- extract coefficients --- coef <- coef(object) # effects of inputs on outputs # q vector \"alpha\": intercepts # p x q matrix \"beta\": slopes  #--- model comparison --- loss <- cv.joinet(Y=Y,X=X) # cross-validated loss # row \"base\": univariate # row \"meta\": multivariate } # }"},{"path":"/reference/joinet.html","id":null,"dir":"Reference","previous_headings":"","what":"Multivariate Elastic Net Regression — joinet","title":"Multivariate Elastic Net Regression — joinet","text":"Implements multivariate elastic net regression.","code":""},{"path":"/reference/joinet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multivariate Elastic Net Regression — joinet","text":"","code":"joinet(   Y,   X,   family = \"gaussian\",   nfolds = 10,   foldid = NULL,   type.measure = \"deviance\",   alpha.base = 1,   alpha.meta = 1,   weight = NULL,   sign = NULL,   ... )"},{"path":"/reference/joinet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multivariate Elastic Net Regression — joinet","text":"Y outputs: numeric matrix \\(n\\) rows (samples) \\(q\\) columns (outputs) X inputs: numeric matrix \\(n\\) rows (samples) \\(p\\) columns (inputs) family distribution: vector length \\(1\\) \\(q\\) entries \"gaussian\", \"binomial\" \"poisson\" nfolds number folds foldid fold identifiers: vector length \\(n\\) entries \\(1\\) nfolds; NULL (balance) type.measure loss function: vector length \\(1\\) \\(q\\) entries \"deviance\", \"class\", \"mse\" \"mae\" (see cv.glmnet) alpha.base elastic net mixing parameter base learners: numeric \\(0\\) (ridge) \\(1\\) (lasso) alpha.meta elastic net mixing parameter meta learners: numeric \\(0\\) (ridge) \\(1\\) (lasso) weight input-output relations: matrix \\(p\\) rows (inputs) \\(q\\) columns (outputs) entries \\(0\\) (exclude) \\(1\\) (include), NULL (see details) sign output-output relations: matrix \\(q\\) rows (\"meta-inputs\") \\(q\\) columns (outputs), entries \\(-1\\) (negative), \\(0\\) (none), \\(1\\) (positive) \\(NA\\) (), NULL (see details) ... arguments passed glmnet","code":""},{"path":"/reference/joinet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Multivariate Elastic Net Regression — joinet","text":"function returns object class joinet. Available methods include predict, coef, weights. slots base meta contain \\(q\\) cv.glmnet-like objects.","code":""},{"path":"/reference/joinet.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Multivariate Elastic Net Regression — joinet","text":"input-output relations: matrix \\(p\\) rows \\(q\\) columns, entry \\(j\\)th row \\(k\\)th column indicates whether \\(j\\)th input may used modelling \\(k\\)th output (\\(0\\) means \"exclude\" \\(1\\) means \"include\"). default (sign=NULL), entries set \\(1\\). output-output relations: matrix \\(q\\) rows \\(q\\) columns, entry \\(l\\)th row \\(k\\)th column indicates \\(l\\)th output may used modelling \\(k\\)th output (\\(-1\\) means negative effect, \\(0\\) means effect, \\(1\\) means positive effect, \\(NA\\) means effect). three short-cuts filling matrix: (1) sign=1 sets entries \\(1\\) (non-negativity constraints). useful pairs outcomes assumed positively correlated (potentially changing sign outcomes). (2) code=NA sets diagonal entries \\(1\\) -diagonal entries NA (constraints). (3) sign=NULL uses Spearman correlation determine entries, \\(-1\\) significant negative, \\(0\\) insignificant, \\(1\\) significant positive correlations. elastic net: alpha.base controls input-output effects, alpha.meta controls output-output effects; lasso renders sparse models (alpha\\(=1\\)), ridge renders dense models (alpha\\(=0\\))","code":""},{"path":"/reference/joinet.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Multivariate Elastic Net Regression — joinet","text":"Armin Rauschenberger Enrico Glaab (2021) \"Predicting correlated outcomes molecular data\" Bioinformatics 37(21):3889–3895. doi:10.1093/bioinformatics/btab576  (Click access PDF.)","code":""},{"path":[]},{"path":[]},{"path":"/reference/predict.joinet.html","id":null,"dir":"Reference","previous_headings":"","what":"Make Predictions — predict.joinet","title":"Make Predictions — predict.joinet","text":"Predicts outcome features stacked model.","code":""},{"path":"/reference/predict.joinet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make Predictions — predict.joinet","text":"","code":"# S3 method for class 'joinet' predict(object, newx, type = \"response\", ...)"},{"path":"/reference/predict.joinet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make Predictions — predict.joinet","text":"object joinet object newx covariates: numeric matrix \\(n\\) rows (samples) \\(p\\) columns (variables) type character \"link\" \"response\" ... arguments (applicable)","code":""},{"path":"/reference/predict.joinet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make Predictions — predict.joinet","text":"function returns predictions base meta learners. slots base meta contain matrix \\(n\\) rows (samples) \\(q\\) columns (variables).","code":""},{"path":"/reference/predict.joinet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make Predictions — predict.joinet","text":"","code":"if (FALSE) { # \\dontrun{ n <- 50; p <- 100; q <- 3 X <- matrix(rnorm(n*p),nrow=n,ncol=p) Y <- replicate(n=q,expr=rnorm(n=n,mean=rowSums(X[,1:5]))) Y[,1] <- 1*(Y[,1]>median(Y[,1])) object <- joinet(Y=Y,X=X,family=c(\"binomial\",\"gaussian\",\"gaussian\")) predict(object,newx=X)} # }"},{"path":"/reference/weights.joinet.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Weights — weights.joinet","title":"Extract Weights — weights.joinet","text":"Extracts coefficients meta learner, .e. weights base learners.","code":""},{"path":"/reference/weights.joinet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Weights — weights.joinet","text":"","code":"# S3 method for class 'joinet' weights(object, ...)"},{"path":"/reference/weights.joinet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Weights — weights.joinet","text":"object joinet object ... arguments (applicable)","code":""},{"path":"/reference/weights.joinet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Weights — weights.joinet","text":"function returns matrix \\(1+q\\) rows \\(q\\) columns. first row contains intercepts, rows contain slopes, effects outcomes row outcomes column.","code":""},{"path":"/reference/weights.joinet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Weights — weights.joinet","text":"","code":"if (FALSE) { # \\dontrun{ n <- 50; p <- 100; q <- 3 X <- matrix(rnorm(n*p),nrow=n,ncol=p) Y <- replicate(n=q,expr=rnorm(n=n,mean=rowSums(X[,1:5]))) object <- joinet(Y=Y,X=X) weights(object)} # }"},{"path":"/news/index.html","id":"joinet-100-2024-09-24","dir":"Changelog","previous_headings":"","what":"joinet 1.0.0 (2024-09-24)","title":"joinet 1.0.0 (2024-09-24)","text":"updated documentation","code":""},{"path":"/news/index.html","id":"joinet-0010-2021-08-05","dir":"Changelog","previous_headings":"","what":"joinet 0.0.10 (2021-08-05)","title":"joinet 0.0.10 (2021-08-05)","text":"CRAN release: 2021-08-09 udpated documentation","code":""},{"path":"/news/index.html","id":"joinet-008-2021-06-16","dir":"Changelog","previous_headings":"","what":"joinet 0.0.8 (2021-06-16)","title":"joinet 0.0.8 (2021-06-16)","text":"CRAN release: 2021-07-16 extension (weights, without constraints)","code":""},{"path":"/news/index.html","id":"joinet-006-2020-11-18","dir":"Changelog","previous_headings":"","what":"joinet 0.0.6 (2020-11-18)","title":"joinet 0.0.6 (2020-11-18)","text":"CRAN release: 2020-11-23 conditional execution","code":""},{"path":"/news/index.html","id":"joinet-005-2020-10-02","dir":"Changelog","previous_headings":"","what":"joinet 0.0.5 (2020-10-02)","title":"joinet 0.0.5 (2020-10-02)","text":"CRAN release: 2020-10-03 updated documentation","code":""},{"path":"/news/index.html","id":"joinet-004-2020-05-06","dir":"Changelog","previous_headings":"","what":"joinet 0.0.4 (2020-05-06)","title":"joinet 0.0.4 (2020-05-06)","text":"added competing models","code":""},{"path":"/news/index.html","id":"joinet-003-2019-11-12","dir":"Changelog","previous_headings":"","what":"joinet 0.0.3 (2019-11-12)","title":"joinet 0.0.3 (2019-11-12)","text":"CRAN release: 2019-11-13 changed glmnet exports","code":""},{"path":"/news/index.html","id":"joinet-002-2019-08-08","dir":"Changelog","previous_headings":"","what":"joinet 0.0.2 (2019-08-08)","title":"joinet 0.0.2 (2019-08-08)","text":"CRAN release: 2019-08-08 performance comparison","code":""},{"path":"/news/index.html","id":"joinet-001-2019-08-03","dir":"Changelog","previous_headings":"","what":"joinet 0.0.1 (2019-08-03)","title":"joinet 0.0.1 (2019-08-03)","text":"CRAN release: 2019-08-03 first submission","code":""}]
